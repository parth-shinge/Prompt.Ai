# ğŸš€ Prompt-Gen (Hybrid + Ranker)

A **hybrid AI prompt generation framework** with:
- âœ¨ **Streamlit UI** for easy usage  
- ğŸ—„ï¸ **SQLite + SQLAlchemy ORM** for storage (Users, Prompts, Choices, Feedback)  
- ğŸ§  **Hybrid ranker** that learns from user choices (TF-IDF & SentenceTransformer embeddings)  
- ğŸ“Š **Evaluation scripts** for model comparison & reproducibility  

---

## ğŸ“‚ Project Structure
- `prompt_generator.py` â†’ Streamlit app (UI, history, admin panel)  
- `database.py` â†’ Database ORM (Users, Prompts, Choices, Feedback)  
- `ranker.py` â†’ Ranker training & inference (TF-IDF + embeddings)  
- `kfold_cv.py` â†’ K-fold experiments, writes `kfold_results.csv`  
- `ranker_retrain.py` â†’ CLI retraining from dataset  
- `evaluation.py` â†’ Accuracy, confusion matrix evaluation  
- `requirements.txt` â†’ Dependencies  

---

## ğŸ”® Model Choices (for Ranker)
- ğŸŸ¢ **all-MiniLM-L6-v2** â†’ Small, fast, great default  
- ğŸ”µ **all-MiniLM-L12-v2** â†’ Larger, more accurate (â‰¥300 examples)  
- ğŸŸ£ **paraphrase-MiniLM-L6-v2** â†’ Good for paraphrase similarity  
- ğŸŸ  **paraphrase-MiniLM-L12-v2** â†’ Larger paraphrase model, better with more data  

ğŸ‘‰ Start with `all-MiniLM-L6-v2` and scale up when dataset grows.  

---

## âš¡ Quick Start
1ï¸âƒ£ Install dependencies  
pip install -r requirements.txt
2ï¸âƒ£ Launch the app  
streamlit run prompt_generator.py

---

## âœ¨ Features

- **Prompt Generation Modes**
  - **Offline**: Deterministic template-based generator (no external API).
  - **Gemini**: Uses Google Gemini (model configured via `GEMINI_MODEL` in Streamlit secrets).
  - **Hybrid**: Generates both Offline + Gemini variants and lets user (or ranker) choose.
  - **Ensemble**: Generates both variants and synthesizes a merged prompt via `ensemble_synthesize`, saved with `model_used="ensemble"`.

- **Admin Panel**
  - **Train Ranker**: Train TFâ€‘IDF or embedding-based LogisticRegression using choice data.
  - **Explain Ranker (SHAP)**: Visualize global feature importance for the TFâ€‘IDF ranker using SHAP values.
  - **Export Data**: Download anonymized JSONL/CSV exports of `Prompt` and `Choice` tables with `user_id` replaced by an HMAC using `ANON_EXPORT_SALT`.
  - **Choices Dataset**: Inspect and download the training data used for the ranker.

---

## ğŸ” Reproducible Experiments

- **Kâ€‘Fold Comparisons**
  - Run crossâ€‘validated comparisons between TFâ€‘IDF and multiple embedding models:
 
  python kfold_cv.py
    - Writes `kfold_results.csv` with perâ€‘embedding model metrics.

- **Ranker Evaluation**
  - Evaluate a trained ranker on the current choices dataset:
 
  python evaluation.py --ranker ranker.pkl --out eval_report.json
    - Produces `eval_report.json` (accuracy & zâ€‘score vs random) and `confusion_matrix.png`.
